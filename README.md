# Overview of ASR software
This github was instantiated with the intent of providing an overview of Systematic Review Software available to researchers. First and foremost, a distinction is made between two types of software products: ones that use machine learning techniques to expedite the abstract screening process of systematic reviews and ones that do not use machine learning techniques. A short summary follows of each software product, which is also summarized in the following table:

# Software products that use machine learning techniques

## Automated Systematic Review

## Rayyan
Rayyanis  the  first  software  product  that  is  discussed.Rayyan takes a set of references as input and yields the subsetof relevant references as output.  Rayyan currently supportsimporting a set of references in RIS, BibTex, CSV, PubMedXML,  Web  of  Science,  and  EndNote  formats  (Ouzzani  etal., 2016).  Moreover, Rayyan uses textual features such asMeSH terms in the form a word cloud, and a user-specifiedlist  of  keywords  that  signify  either  inclusion  or  exclusion.All words are represented by their stems and these featuresare  used  as  input  to  a  support  vector  machine  (Ouzzani  etal., 2016).  When it comes to active learning, Rayyan usesa  5-star system  to  present  the relevance  of  each reference,and by default the references are ranked based on their levelof relevance, presenting those references that are most likelyto be relevant on top.   The stopping criterion Rayyan usesis when all of the references have been labeled or when no further improvements to the model can be made (Ouzzani etal., 2016).

## Abstrackr
Abstrackr is  another  software  product  that  aims  to  aidreviewers by automating systematic reviews using machinelearning techniques. Abstrackr accepts a set of PubMed IDs,an RIS file or a file delimited by tabs (Wallace et al., 2012).Similar to Rayyan, both the labels for the features, words orn-grams,  and the labels for citations are fed into a supportvector machine (Wallace et al., 2012). Abstrackr also makesuse of the dual supervision paradigm,  enabling researchersto  specify  sets  of  keywords  that  may  indiciate  relevanceor  irrelevance,  thereby  leveraging  their  domain  knowledge(Wallace et al., 2012).Next,  the  lead  reviewer  is  prompted  to  select  the  ac-tive  learning  strategy  they  wish  to  use:   certainty-based,uncertainty-based or random sampling.  This strategy deter-mines the order in which the references are presented. How-ever, in active learning it is frequently assumed that we aredealing  with  one  oracular  annotator  who  makes  no  errorswhen providing classifications (Wallace et al., 2012). This isan assumption which may not always be satisfied.  Multipleannotators with different levels of expertise may be workingon the same dataset.   A new component in active learning3https://rayyan.qcri.org4http://abstrackr.cebm.brown.edu
4A. HARKEMAis  proposed,  in  which  most  labeling  is  done  by  less  expe-rienced reviewers, and whenever they encounter referencesthat they experience as more troublesome to classify, thesereferences are re-assigned to the more experienced reviewersto ensure rapid yet highly accurate classifications (Wallace etal., 2012).  Finally, the stopping criterion Abstrackr has im-plemented is that the process stops when the model predictsthat none of the remaining unlabeled references are possibly relevant (Wallace et al., 2012)

## Swift-ActiveScreener
Thirdly,  SWIFT-ActiveScreener also  uses  text  mining and machine learning techniques to expedite the systematic review process. It accepts EndNote, Mendeley, Zotaro, andSWIFTReview  files,  a  set  of  PubMed  IDs  or  an  exportedXML  file  of  a  PubMed  search  as  input,  and  it  outputs  aset  of  relevant  references  (Howard,  2009).   For  its  textual features,SWIFT-ActiveScreener    uses    a    bag-of-wordsmodel,  including  bi-  and  trigram  representations  and  its words are represented by their respective stems.  Moreover,SWIFT-ActiveScreener    uses    length-normalized    TF-IDFrepresentations  and  latent  dirichlet  allocation  (Blei  et  al.,2003) for clustering purposes (Howard et al., 2016).Subsequently,  these  word  score  features  and  topic  fea-tures are fed into a logistic regression model.   Using thesefeatures  and  a  set  of  manually  screened  references  it  willtrain a logistic regression model which will output a 1 or a 0for an inclusion or an exclusion respectively (Howard et al.,2016). As of yet, further active learning has not been imple-mented, although it is stated that implementing active learn-ing is being looked into as a means of improving SWIFT-ActiveScreener’s classification model (Howard et al., 2016).Ultimately,  it  was  ascertained  again  that  finding  a  suitablestopping  criterion  for  when  to  signal  the  reviewer  to  stopscreening is not an easy task, and that finding a suitable so-lution is being looked into (Howard et al., 2016).  SWIFT-ActiveScreener currently uses random sampling, and on topof this an extra model has been implemented to predict theremaining amount of classifications to be made (Przybyła etal., 2018)

## RobotAnalyst
RobotAnalyst\footnote{http://abstrackr.cebm.brown.edu} currently accepts a set of references in RIS format. A number of these references have to be classified manually in order for RobotAnalyst to train an initial model \cite{robotanalyst2018}. For its textual features, each document is passed through a Part-Of-Speech tagger which also performs stemming on all words. Subsequently, similar to SWIFT-ActiveScreener, latent dirichlet allocation \cite{andrewlda} is used for topic modelling purposes. Further clustering is done using spectral clustering \cite{spectral} which also uses TF-IDF scores. These feature representations are ultimately fed into a support vector machine.

## Colandr
Colandr\footnote{https://www.colandrapp.com} \cite{colandrpaper} aims to assist reviewers with screening references by taking in a set of references in BibTeX format. Colandr requires the manual classification of 10 references as included before it will train an initial model. For its machine learning implementation, Colandr makes use of the word2vec model \cite{word2vec, efficientvecs}. The word2vec model is used to learn word vector representations which can subsequently be used to look for patterns based on the search terms. Colandr then uses a linear model to learn
which combinations of these vectors indicate relevant references \cite{colandrpaper, robotanalyst2018}. Similar in a sense to the way in which active learning is employed by Rayyan, Abstrackr, and RobotAnalyst, Colandr uses the reviewer's classifications to improve the model and to calculate the references' predicted relevance to determine which reference to present next using certainty-based active learning. Currently it is not clear what stopping criterion Colandr uses for this active learning cycle so the decision is left to the reviewer \cite{colandrpaper}.

## EPPI-Reviewer
EPPI-Reviewer\footnote{https://eppi.ioe.ac.uk/cms/er4} \cite{eppicite} is a text-mining tool that takes in a set of references in a variety of formats which can be converted to the accepted RIS Format by using their RIS export utility. EPPI-Reviewer is then able to apply document clustering by using Lingo3G clustering. Next, by using automatic term recognition \cite{atr}, EPPI-reviewer finds terms in prior included references which can be used to retrieve other relevant references \cite{eppimining}. Active learning is implemented for priority screening and the remaining references are ordered by their potential relevance. The stopping criterion is left to the reviewer.\footnote{At our inquiry, the information on the way EPPI-Reviewer has applied active learning and their particular stopping criterion was provided by the authors.}

# Software products that do not use machine learning techniques
